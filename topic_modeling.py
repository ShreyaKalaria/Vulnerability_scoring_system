# -*- coding: utf-8 -*-
"""
Created on Sun Sep 25 18:47:25 2022

@author: arman hossain

!pip install transformers
!pip install spacy
!python -m spacy download en_core_web_sm
!pip uninstall gensim
!pip install gensim==3.8.3
!pip install pyLDAvis==2.1.2
!pip install nltk


!python topic_modeling.py
"""
# %% import section
import re
# import numpy as np
import pandas as pd
from pprint import pprint
import pickle
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# spacy for lemmatization
import spacy

import pyLDAvis
import pyLDAvis.gensim # don't skip this
# import matplotlib.pyplot as plt
# matplotlib inline

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

class Topics:
    def __init__(self):
        self.lda_model = ""
        self.optimal_topic = 20
        self.id2word = ""
        self.bigram_mod = ""
        self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
    def sent_to_words(self,sentences):
        for sentence in sentences:
            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations
    
    # Define functions for stopwords, bigrams, trigrams and lemmatization
    def remove_stopwords(self,texts):
        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
    
    def make_bigrams(self,texts):
        return [self.bigram_mod[doc] for doc in texts]
    
    # def make_trigrams(texts):
    #     return [trigram_mod[bigram_mod[doc]] for doc in texts]
    
    def lemmatization(self,texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
        """https://spacy.io/api/annotation"""
        texts_out = []
        for sent in texts:
            doc = self.nlp(" ".join(sent)) 
            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
        return texts_out

    def preprocess(self,data,from_model_trainer=False):
        # print("hello i am here",from_model_trainer)
        data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]
        # Remove new line characters
        data = [re.sub('\s+', ' ', sent) for sent in data]
        # Remove distracting single quotes
        data = [re.sub("\'", "", sent) for sent in data]
        data_words = list(self.sent_to_words(data)) #[['receipt', 'of', 'malformed', 'packet', 'on', 'mx'...],[]]
        if from_model_trainer==True:
            # print("hello i am here",from_model_trainer)
            bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
            # trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  
            # Faster way to get a sentence clubbed as a trigram/bigram
            self.bigram_mod = gensim.models.phrases.Phraser(bigram)
            # trigram_mod = gensim.models.phrases.Phraser(trigram)
            
            with open('./epss_models/bigram_mod.pkl', 'wb') as fp:
                pickle.dump(self.bigram_mod, fp)
        
        
        # Remove Stop Words
        data_words_nostops = self.remove_stopwords(data_words)
        
        # Form Bigrams
        data_words_bigrams = self.make_bigrams(data_words_nostops)
        
        # Do lemmatization keeping only noun, adj, vb, adv
        data_lemmatized = self.lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

        return data_lemmatized
    
    def load_model(self):
        if self.lda_model == "":
            self.lda_model = gensim.models.wrappers.LdaMallet.load("./epss_models/lda_model.bin")
        if self.bigram_mod == "":
            with open('./epss_models/bigram_mod.pkl', 'rb') as f:
                self.bigram_mod = pickle.load(f)
        pprint(self.lda_model.show_topics(formatted=False, num_topics = self.optimal_topic, num_words=5))
                
    def topic_score(self,corpus):
        self.load_model()
        topics_data = self.lda_model[corpus]

        val_df_ = pd.DataFrame(topics_data)
        val_df_.columns = ["Topic "+ str(i+1) for i in range(self.optimal_topic)]
        
    def train_model(self,data_lemmatized):
        # Create Dictionary
        self.id2word = corpora.Dictionary(data_lemmatized)

        # Term Document Frequency
        corpus = [self.id2word.doc2bow(text) for text in data_lemmatized]
        
        # !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
        # ! unzip /epss_models/mallet-2.0.8.zip

        
        mallet_path = './epss_models/mallet-2.0.8/bin/mallet' # update this path

        self.lda_model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=self.optimal_topic, id2word=self.id2word, iterations=1000)
        
        self.lda_model.save("./epss_models/lda_model.bin")
        return self.topic_score(corpus)
    
    def generate_model(self,descriptions):
        return self.train_model(self.preprocess(descriptions.values.tolist(),True))
    

if __name__ == '__main__':
        
    data = pd.read_csv('all_data_2019.csv')
    topics = Topics()
    topics.generate_model(data.description)
# sorted(lda_model[corpus[233]], key=lambda x:x[1], reverse=True)
# lda_model[corpus[233]]

# d = lda_model[corpus[233]]
# df_ = pd.DataFrame(d)

# dff = df_.transpose()
# val_df = dff.drop([0])
# val_df = val_df.reset_index()

# val_df.columns = ["Topic "+ str(i) for i in range(optimal_topic)]

# final_df = pd.concat([df, val_df_], axis=1)
# final_df.to_csv('/content/drive/MyDrive/data_nvd/all_with_topics.csv', header=True, index=False)












































