# -*- coding: utf-8 -*-
"""
Created on Sun Sep 25 18:47:25 2022

@author: arman hossain

!pip install transformers
!pip install spacy
!python -m spacy download en_core_web_sm
!pip uninstall gensim
!pip install gensim==3.8.3
!pip install pyLDAvis==2.1.2
!pip install nltk

!python topic_modeling.py
"""
# %% import section
import re
# import numpy as np
import pandas as pd
# from pprint import pprint
import pickle
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
# from gensim.models import CoherenceModel

from utilities import read_csv_dataset

# spacy for lemmatization
import spacy

# import pyLDAvis
# import pyLDAvis.gensim # don't skip this
# import matplotlib.pyplot as plt
# matplotlib inline

# import nltk
# nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

class Topics:
    def __init__(self):
        self.lda_model = ""
        self.optimal_topic = 20
        self.id2word = ""
        self.bigram_mod = ""
        self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
    def sent_to_words(self,sentences):
        for sentence in sentences:
            yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations
    
    # Define functions for stopwords, bigrams, trigrams and lemmatization
    def remove_stopwords(self,texts):
        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
    
    def make_bigrams(self,texts):
        # for doc in texts:
        #     print(doc)
        
        return [self.bigram_mod[doc] for doc in texts]
    
    # def make_trigrams(texts):
    #     return [trigram_mod[bigram_mod[doc]] for doc in texts]
    
    def lemmatization(self,texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
        """https://spacy.io/api/annotation"""
        texts_out = []
        for sent in texts:
            doc = self.nlp(" ".join(sent)) 
            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
        
        return texts_out
    def save_bigram(self,data_words):
        with open('./epss_models/bigram_mod.pkl', 'wb') as fp:
            pickle.dump(self.bigram_mod, fp)
            
    def preprocess(self,data,save): #list of list
        # print("hello i am here",from_model_trainer)
        data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]
        # Remove new line characters
        data = [re.sub('\s+', ' ', sent) for sent in data]
        # Remove distracting single quotes
        data = [re.sub("\'", "", sent) for sent in data]
        data_words = list(self.sent_to_words(data)) #[['receipt', 'of', 'malformed', 'packet', 'on', 'mx'...],[]]
        bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
        self.bigram_mod = gensim.models.phrases.Phraser(bigram)
        if save: self.save_bigram(data_words)
        # Remove Stop Words
        data_words_nostops = self.remove_stopwords(data_words)
        
        # return data_words_nostops
        
        data_words_bigrams = self.make_bigrams(data_words_nostops)
        
        # # Do lemmatization keeping only noun, adj, vb, adv
        data_lemmatized = self.lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
        if save:    
            with open('./epss_models/lamma.pkl', 'wb') as fp:
                pickle.dump(data_lemmatized, fp)
        return data_lemmatized
    
    def load_model(self):
        self.lda_model = gensim.models.wrappers.LdaMallet.load("./epss_models/lda_model.bin")
        with open('./epss_models/bigram_mod.pkl', 'rb') as f:
            self.bigram_mod = pickle.load(f) #bigram generated by our own
            
        with open('./epss_models/id2word.pkl', 'rb') as f:
            self.id2word = pickle.load(f)

        # pprint(self.lda_model.show_topics(formatted=False, num_topics = self.optimal_topic, num_words=5))
        
    def get_corpus(self,data_lemmatized):
        return [self.id2word.doc2bow(text) for text in data_lemmatized]
    
    def topic_score(self,corpus):
        topics_data = self.lda_model[corpus]

        val_df_ = pd.DataFrame(topics_data)
        val_df_.columns = ["Topic "+ str(i+1) for i in range(self.optimal_topic)]
        return val_df_
        
        
    def train_model(self,data_lemmatized):
        # Create Dictionary
        self.id2word = corpora.Dictionary(data_lemmatized)

        # Term Document Frequency
        corpus = [self.id2word.doc2bow(text) for text in data_lemmatized]
        
        # !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
        # ! unzip /epss_models/mallet-2.0.8.zip
        # print(corpus)
        
        mallet_path = './epss_models/mallet-2.0.8/bin/mallet' # update this path

        self.lda_model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=self.optimal_topic, id2word=self.id2word, iterations=1000)
        
        self.lda_model.save("./epss_models/lda_model.bin")
        return self.topic_score(corpus)
    
    def generate_model(self,descriptions):
        return self.train_model(self.preprocess(descriptions.values.tolist(),True))
    

if __name__ == '__main__':
    # experiments
    # data_words = read_pickle('./epss_models/bigram_mod.pkl')
    # data_words_nostops = topics.remove_stopwords(data_words)
    # end experiment
    data = read_csv_dataset('./data/2016_22m_nvd.csv')
    topics = Topics()
    val_df_ = topics.generate_model(data['description'])
    # topics.load_model()
    # data = data.description.values.tolist()

    # lamma = topics.preprocess(data)
    
    # print(lamma[-3])
    # print(lamma[-2])
    # print(lamma[-1])
    # corpus = topics.get_corpus(lamma)
    # print(topics.topic_score(corpus))
    # print(topics.lda_model[''])
    # print(topics.lda_model[[(314, 2), (319, 1), (323, 1), (324, 1), (325, 1), (653, 2), (654, 1)]])
    # print(topics.preprocess(data.description.values.tolist(),True))
    
    # with open('./epss_models/lamma.pkl', 'rb') as f:
    #     data_lemmatized = pickle.load(f)
    
    # # Create Dictionary
    # id2word = corpora.Dictionary(data_lemmatized)
    
    # # Term Document Frequency
    # # corpus = [id2word.doc2bow(text) for text in data_lemmatized]
    # with open('./epss_models/corpus.pkl', 'rb') as f:
    #     corpus = pickle.load(f)
    
    
# sorted(lda_model[corpus[233]], key=lambda x:x[1], reverse=True)
# lda_model[corpus[233]]

# d = lda_model[corpus[233]]
# df_ = pd.DataFrame(d)

# dff = df_.transpose()
# val_df = dff.drop([0])
# val_df = val_df.reset_index()

# val_df.columns = ["Topic "+ str(i) for i in range(optimal_topic)]

# final_df = pd.concat([df, val_df_], axis=1)
# final_df.to_csv('/content/drive/MyDrive/data_nvd/all_with_topics.csv', header=True, index=False)












































