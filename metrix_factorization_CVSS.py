# -*- coding: utf-8 -*-
"""
Created on Sat Dec  3 15:57:52 2022

@author: md arman hossain
"""

import pandas as pd
# from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
# from sklearn.model_selection import train_test_split
from sklearn import metrics
# import matplotlib.pyplot as plt
import numpy as np
# from sklearn.preprocessing import normalize
from scipy.sparse.linalg import svds
from epss_dataset_generator import preprocess

def update(A,B,X,K):
  eps = 2.2*10**-16
  for k in range(K):
    A[A < eps] = eps
    B[B < eps] = eps
    A = np.multiply(A, X.dot(B.transpose())/A.dot(B).dot(B.transpose()))
    B = np.multiply(B,A.transpose().dot(X)/A.transpose().dot(A).dot(B))
    
  return A,B

def projection_B(X,B):
  A = X.dot(B.transpose())
  return A
def get_features(X,num_components_k=2,iteration = 4):
  u, s, v = svds(X, k=num_components_k)
  A0 = np.absolute(u)
  B0 = np.absolute(v)
  A,B = update(A0,B0,X,iteration)
  # A = projection_B(X,B)
  return A,B

def valid_index(lst):
    gotZero = False
    first = 0
    for i in range (0,len(lst)):
        if lst[i]==-1:
            if gotZero:
                return first,i
        elif not gotZero:
            gotZero = True
            first = i
    return first, len(lst)

def precision_recall_fmeasure(confusion_matrix):
    type(confusion_matrix)
    true = 0
    for i in range(len(confusion_matrix[0])):
        true+=confusion_matrix[i][i]
        pre = confusion_matrix[i][i]/np.sum(confusion_matrix[:,i])
        recall = confusion_matrix[i][i]/np.sum(confusion_matrix[i,:])
        print('class: ',i+1,' precision: ',"{:.2f}".format(pre), ' recall: ',"{:.2f}".format(recall), ' f measure: ',"{:.2f}".format(2*(pre*recall)/(pre+recall)))
    print('overall accuracy: ',true/np.sum(confusion_matrix))
    

def test(data,X,epss,num_components_k=4):
    # num_components_k=8
    iteration=100
    y_test=''
    y_pred=''
    print('with features ',num_components_k,' Iteration ',iteration)
    for i in range(2016,2023):
        a = data.index.str.find('CVE-'+str(i))
        start,end = valid_index(a) # end excludin
        # pos = int(i*col(X)/6)
        pos = int(start+(end-start)/2)
        X_train = X[0:pos,:]
        y_train = epss[0:pos] 
        X_test = X[pos:end,:]
        y_test = epss[pos:end]
        A,B = get_features(X_train,num_components_k=num_components_k,iteration = iteration)
        X_train = projection_B(X_train,B)
        X_test = projection_B(X_test,B)
          
        # Create Decision Tree classifer object
        clf = DecisionTreeClassifier()
        clf = clf.fit(X_train,y_train)
        y_pred = clf.predict(X_test)
        # print(np.unique(y_pred))
        # print(np.unique(y_test))
        confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
        print('confusion_matrix')
        print(confusion_matrix)
        precision_recall_fmeasure(confusion_matrix)
        # cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [True, True])
        # cm_display.plot()
        # plt.show()
        
        
        print(str(i)+" Accuracy: ",metrics.accuracy_score(y_test, y_pred),' Train data ',len(y_train),' Test data ', len(y_test))
    
def test_without_factorization(data,X,epss):
    y_test=''
    y_pred=''
    print('without factorization ')
    for i in range(2016,2023):
        a = data.index.str.find('CVE-'+str(i))
        start,end = valid_index(a) # end excludin
        # pos = int(i*col(X)/6)
        pos = int(start+(end-start)/2)
        X_train = X[0:pos,:]
        y_train = epss[0:pos] 
        X_test = X[pos:end,:]
        y_test = epss[pos:end]
        # A,B = get_features(X_train,num_components_k=num_components_k,iteration = iteration)
        # X_train = projection_B(X_train,B)
        # X_test = projection_B(X_test,B)
          
        # Create Decision Tree classifer object
        clf = DecisionTreeClassifier()
        clf = clf.fit(X_train,y_train)
        y_pred = clf.predict(X_test)
        # print(np.unique(y_pred))
        # print(np.unique(y_test))
        confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
        print(confusion_matrix)
        precision_recall_fmeasure(confusion_matrix)
        # cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])
        # cm_display.plot()
        # plt.show()
        print(str(i)+" Accuracy: ",metrics.accuracy_score(y_test, y_pred),' Train data ',len(y_train),' Test data ', len(y_test))

def col(X):
  return len(X[:,0])
def row(X):
  return len(X[0,:])

def convert_np(data):
    X = data.to_numpy()
    # print(col(X),row(X))
    X = X[~np.isnan(X).any(axis=1), :] # deleting nan
    # print(col(X),row(X))
    return X

def get_label(X):
    epss = X[:,len(X[0])-1]
    # epss
    X = np.delete(X,len(X[0])-1,1) # deleting last column
    print(col(X),row(X))
    return X,epss
# %% training CVSS

data = pd.read_csv('./data/data2/cvss_dataset_final_epss.csv',index_col = 0)

data = data[['attackVector','attackComplexity','privilegesRequired','userInteraction','scope','confidentialityImpact','integrityImpact','availabilityImpact','cvss']]
data.dropna(inplace=True)
X1 = convert_np(data)
X,cvss = get_label(X1)
# test(data,X,cvss,5)
test_without_factorization(data,X,cvss)



#%% training EPSS
preprocess(a=0.00949,b=0.019,cvss=False)
data = pd.read_csv('./data/data2/epss_dataset_final_epss.csv',index_col = 0)

data.dropna(inplace=True)
X1 = convert_np(data)
X,cvss = get_label(X1)
test(data,X,cvss,15)
# test_without_factorization(data,X,cvss)

#%% actual vs prediction
from epss_dataset_generator import cvss_thresholding

# data = pd.read_csv('./data/actual_vs_prediction_score.csv',index_col = 0)
data = data[data.baseScore_pred>0]
data = cvss_thresholding(data,'baseScore_pred')
data = cvss_thresholding(data,'baseScore')
print('confusion_matrix: ')
confusion_matrix = metrics.confusion_matrix(data.baseScore, data.baseScore_pred)

precision_recall_fmeasure(confusion_matrix)
len(data)
3063/.20
15315+3063
# %% actual vs prediction numeric metrics
from sklearn.metrics import mean_squared_error , mean_absolute_percentage_error, mean_absolute_error, r2_score
import math

data = pd.read_csv('./data/actual_vs_prediction_score.csv',index_col = 0)
data = data[data.baseScore_pred>0]
mse = mean_squared_error(data.baseScore,data.baseScore_pred)
rmse = math.sqrt(mse)
mae = mean_absolute_error(data.baseScore,data.baseScore_pred)
mape = mean_absolute_percentage_error(data.baseScore,data.baseScore_pred)
r_square = r2_score(data.baseScore,data.baseScore_pred)




