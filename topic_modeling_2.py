# -*- coding: utf-8 -*-
"""
Created on Mon Sep 26 21:15:28 2022

@author: arman hossain

!python topic_modeling_2.py
"""

import pickle

import pandas as pd
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
import nltk
# nltk.download('stopwords')
import re
import warnings
nltk.download('stopwords')
from nltk.corpus import stopwords
# spacy for lemmatization
import spacy

stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use']) 


# ------------------------------------
bigram_mod = []
id2word = []
corpus = []
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
# with open('./epss_models/bigram_mod.pkl', 'rb') as f:
#     bigram_mod = pickle.load(f) #bigram generated by our own

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations


def save_bigram(data_words):
    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
    bigram_mod = gensim.models.phrases.Phraser(bigram)
    with open('./epss_models/bigram_mod.pkl', 'wb') as fp:
        pickle.dump(bigram_mod, fp)
        
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    
    return texts_out
def preprocess(data): #list of list
    # print("hello i am here",from_model_trainer)
    data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]
    # Remove new line characters
    data = [re.sub('\s+', ' ', sent) for sent in data]
    # Remove distracting single quotes
    data = [re.sub("\'", "", sent) for sent in data]
    data_words = list(sent_to_words(data)) #[['receipt', 'of', 'malformed', 'packet', 'on', 'mx'...],[]]
    # self.save_bigram(data_words)
    # Remove Stop Words
    data_words_nostops = remove_stopwords(data_words)
    
    # return data_words_nostops
    
    data_words_bigrams = make_bigrams(data_words_nostops)
    
    # # Do lemmatization keeping only noun, adj, vb, adv
    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
    with open('./epss_models/lamma.pkl', 'wb') as fp:
        pickle.dump(data_lemmatized, fp)
    return data_lemmatized
def get_corpus(data_lemmatized):
    id2word = corpora.Dictionary(data_lemmatized)
    with open('./epss_models/id2word.pkl', 'wb') as fp:
        pickle.dump(id2word, fp)
    corpus = [id2word.doc2bow(text) for text in data_lemmatized]
    with open('./epss_models/corpus.pkl', 'wb') as fp:
        pickle.dump(corpus, fp)
    return corpus

def load():
    # self.lda_model = gensim.models.wrappers.LdaMallet.load("./epss_models/lda_model.bin")

    with open('./epss_models/bigram_mod.pkl', 'rb') as f:
        global bigram_mod
        bigram_mod = pickle.load(f) #bigram generated by our own
        
    with open('./epss_models/id2word.pkl', 'rb') as f:
        global id2word
        id2word = pickle.load(f)
    with open('./epss_models/corpus.pkl', 'rb') as f:
        global corpus
        corpus = pickle.load(f)

    # pprint(self.lda_model.show_topics(formatted=False, num_topics = self.optimal_topic, num_words=5))
    


data = pd.read_csv('only_desc_1029.csv')
# train_corpus, train_id2word, bigram_train = get_corpus(data.description)
data = data.description.values.tolist()
# lamma = preprocess(data)
# corpus = get_corpus(lamma)
load()
# print(lamma[-1])
# print(corpus[-1])
with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    lda_train = gensim.models.ldamulticore.LdaMulticore(
                            corpus=corpus,
                            num_topics=20,
                            id2word=id2word,
                            chunksize=100,
                            workers=7, # Num. Processing Cores - 1
                            passes=50,
                            eval_every = 1,
                            per_word_topics=False)
    lda_train.save('lda_train.model')
    
# lda_train = gensim.models.ldamulticore.LdaMulticore.load("./epss_models/lda_train.model")
print(lda_train.print_topics(20, num_words=5))



#%%
# data = pd.read_csv('all_data_2019.csv')
# descriptions = data.description.values.tolist()
# new_des = []
# for item in descriptions:
#     if item.find("** REJECT **") < 0:
#         new_des.append(item)

# df = pd.DataFrame()
# df["description"] = new_des

# df.to_csv("only_desc_1029.csv")
