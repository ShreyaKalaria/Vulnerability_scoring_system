# -*- coding: utf-8 -*-
"""
Created on Sat Aug  6 15:34:15 2022

@author: IIT
"""
#%%
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer



#%% 
data = pd.read_csv('../data/output/av.csv')

tokenizer = BertTokenizer.from_pretrained(
    './model/bert_uncased_L-4_H-512_A-8',
    do_lower_case = True,
    )
print(tokenizer)

#%% lebel processing

label = data['attackVector']

encoder = preprocessing.LabelEncoder()
label_val = encoder.fit_transform(label)

#%%

# token_id = []
# attention_masks = []

# def preprocessing(input_text, tokenizer):
#   '''
#   Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:
#     - input_ids: list of token ids
#     - token_type_ids: list of token type ids
#     - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).
#   '''
#   return tokenizer.encode_plus(
#                         input_text,
#                         add_special_tokens = True,
#                         max_length = 128,
#                         pad_to_max_length = True,
#                         return_attention_mask = True,
#                         return_tensors = 'pt'
#                    )


# for sample in data['value']:
#   encoding_dict = preprocessing(sample, tokenizer)
#   token_id.append(encoding_dict['input_ids']) 
#   attention_masks.append(encoding_dict['attention_mask'])


# token_id = torch.cat(token_id, dim = 0)
# attention_masks = torch.cat(attention_masks, dim = 0)

#%%
labels = torch.tensor(label_val)

#%% model trianing


# val_ratio = 0.2
# # Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf
# batch_size = 128

# # Indices of the train and validation splits stratified by labels
# train_idx, val_idx = train_test_split(
#     np.arange(len(labels)),
#     test_size = val_ratio,
#     shuffle = True,
#     stratify = labels)

# Train and validation sets
# train_set = TensorDataset(token_id[train_idx], 
#                           attention_masks[train_idx], 
#                           labels[train_idx])

# val_set = TensorDataset(token_id[val_idx], 
#                         attention_masks[val_idx], 
#                         labels[val_idx])

# Prepare DataLoader
# train_dataloader = DataLoader(
#             train_set,
#             sampler = RandomSampler(train_set),
#             batch_size = batch_size
#         )

# validation_dataloader = DataLoader(
#             val_set,
#             sampler = SequentialSampler(val_set),
#             batch_size = batch_size
#         )

#%% 
import collections
label_num = len(collections.Counter(label_val))
X_train, X_test, y_train, y_test = train_test_split(data['value'], label_val, test_size=0.2, random_state=42)
X_train_data = X_train.values.tolist()
X_test_data = X_test.values.tolist()

X_train_ = tokenizer(X_train_data, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
print(X_train_)

#%%
X_test_ = tokenizer(X_test_data, return_tensors="pt", padding="max_length", max_length=128, truncation=True)

#%%
print(X_train_['input_ids'].shape)
# train_set = tokenizer(text, max_length=128)
# Load the BertForSequenceClassification model
# model = BertForSequenceClassification.from_pretrained(
#     './model/bert_uncased_L-4_H-512_A-8',
#     num_labels = label_num
# )

# # Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf
# optimizer = torch.optim.AdamW(model.parameters(), 
#                               lr = 5e-5,
#                               eps = 1e-08
#                               )

# # Run on GPU
# model.cuda()

# %%
import torch

class Dataset(torch.utils.data.Dataset):
    """
    Class to store the data as PyTorch Dataset
    """
    
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
        
    def __getitem__(self, idx):
        # an encoding can have keys such as input_ids and attention_mask
        # item is a dictionary which has the same keys as the encoding has
        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    
    def __len__(self):
        return len(self.labels)
        

print(Dataset.__doc__)

#%%
num_of_epochs = 10
learning_rate = 5e-5
# model = AutoModelForSequenceClassification.from_pretrained('./model/bert_uncased_L-4_H-512_A-8', num_labels=4)

# %%

from numpy import mean
from torch import nn
from torch.utils.data import DataLoader
from transformers import AdamW

# Dataset & dataloader
train_dataset = Dataset(X_train_, y_train)
val_dataset = Dataset(X_test_, y_test)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)
print('Created train & val datasets.')

# device (turn on GPU acceleration for faster execution)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

# model
model = AutoModelForSequenceClassification.from_pretrained('./model/bert_uncased_L-4_H-512_A-8', num_labels=4)
model.to(device)

#optimizer
optimizer = AdamW(model.parameters(), lr=learning_rate)
#%%
# training_args = TrainingArguments(
#     output_dir="./results",
#     per_device_train_batch_size=128,
#     per_device_eval_batch_size=64,
#     num_train_epochs=6,
#     evaluation_strategy='epoch',
#     save_strategy='epoch'
    
    
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=X_train_,
#     eval_dataset=X_test_,
#     tokenizer=tokenizer,
#     # data_collator=data_collator
# )

# trainer.train()


def train(dataloader, model, optimizer):
    """Method to train the model"""
    
    model.train()
    
    epoch_loss = 0
    size = len(dataloader.dataset)
    
    for i, batch in enumerate(dataloader):  
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].type(torch.LongTensor).to(device)
        
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

        optimizer.zero_grad()
        loss = outputs.loss
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()

        
    print('Training loss: {:.3f}'.format(epoch_loss / size))
            
print(train.__doc__)

def test(dataloader, model):
    """Method to test the model's accuracy and loss on the validation set"""
    
    model.eval()
    
    size = len(dataloader.dataset)
    test_loss, accuracy = 0, 0
    
    with torch.no_grad():
        for batch in dataloader:
            X, y = batch['input_ids'].to(device), batch['labels'].type(torch.LongTensor).to(device)
            pred = model(X, labels=y)
            
            test_loss += pred.loss
            accuracy += (pred.logits.softmax(1).argmax(1) == y).type(torch.float).sum().item()
            
        test_loss /= size
        accuracy /= size
        
        print("Test loss: {:.3f}, accuracy: {:.3f}%".format(test_loss, accuracy * 100))
        
print(test.__doc__)
# %%
print(model)
# %%
from tqdm.auto import tqdm

tqdm.pandas()

for name, param in model.named_parameters():
    if 'classifier' not in name: # classifier layer
        param.requires_grad = False
    else:
        param.requires_grad = True
# print(model)        
   #%%     
for i in tqdm(range(num_of_epochs//2)):
    print("Epoch: #{}".format(i+1))
    train(train_loader, model, optimizer)
    test(val_loader, model)
    
print("After Unfreezing the layer......................")

for name, param in model.named_parameters():
		param.requires_grad = True
        
for i in tqdm(range(num_of_epochs//2, num_of_epochs)):
    print("Epoch: #{}".format(i+1))
    train(train_loader, model, optimizer)
    test(val_loader, model)






































