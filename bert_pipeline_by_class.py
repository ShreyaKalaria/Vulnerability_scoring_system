# -*- coding: utf-8 -*-
"""
Created on Sun Aug  7 19:42:59 2022

@author: IIT
"""
import torch
from torch.utils.data import  DataLoader
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
from sklearn import preprocessing

# from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification
from tqdm.auto import tqdm
from transformers import AdamW






class Dataset(torch.utils.data.Dataset):
    """
    Class to store the data as PyTorch Dataset
    """
    
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
        
    def __getitem__(self, idx):
        # an encoding can have keys such as input_ids and attention_mask
        # item is a dictionary which has the same keys as the encoding has
        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    
    def __len__(self):
        return len(self.labels)
        

print(Dataset.__doc__)



class Predictor:
    
    """
    Class to for holding predictor object
    
    """
    
    def __init__(self, tokenizer='./model/bert_uncased_L-4_H-512_A-8', num_of_epochs = 10,learning_rate = 5e-5):
        self.num_of_epochs = num_of_epochs
        self.learning_rate = learning_rate
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer,do_lower_case = True) 
        self.encoder = preprocessing.LabelEncoder()
        # self.device = torch.device('cpu')
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        
    def train(self, model, optimizer):
        """Method to train the model"""
        dataloader = self.train_loader
        model.train()
        
        epoch_loss = 0
        size = len(dataloader.dataset)
        
        for i, batch in enumerate(dataloader):  
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].type(torch.LongTensor).to(self.device)
            
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
    
            optimizer.zero_grad()
            loss = outputs.loss
            print(loss)
            epoch_loss += loss.item()
            loss.backward()
            optimizer.step()
    
            
        print('Training loss: {:.3f}'.format(epoch_loss / size))
                
    print(train.__doc__)
    
    def test(self, model):
        """Method to test the model's accuracy and loss on the validation set"""
        dataloader = self.test_loader
        model.eval()
        
        size = len(dataloader.dataset)
        test_loss, accuracy = 0, 0
        
        with torch.no_grad():
            for batch in dataloader:
                X, y = batch['input_ids'].to(self.device), batch['labels'].type(torch.LongTensor).to(self.device)
                pred = model(X, labels=y)
                
                test_loss += pred.loss
                accuracy += (pred.logits.softmax(1).argmax(1) == y).type(torch.float).sum().item()
                
            test_loss /= size
            accuracy /= size
            
            print("Test loss: {:.3f}, accuracy: {:.3f}%".format(test_loss, accuracy * 100))    
    def prepare_data(self, descriptions,label):
        
        # label processing
        label = self.encoder.fit_transform(label)
        # labels = torch.tensor(label)
        X_train, X_test, y_train, y_test = train_test_split(descriptions, label, test_size=0.2, random_state=42)
        
        # converting to list
        X_train = X_train.values.tolist()
        X_test = X_test.values.tolist()
        
        # some saving
        # self.X_train = X_train
        # self.X_test = X_test
        # self.y_train = y_train
        # self.y_test = y_test
        
        # tokenize
        X_train = self.tokenizer(X_train, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        X_test = self.tokenizer(X_test, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        
        
        #dataloader
        self.train_loader = DataLoader(Dataset(X_train, y_train), batch_size=64, shuffle=True)
        self.test_loader = DataLoader(Dataset(X_test, y_test), batch_size=64, shuffle=True)
        
        
    
    def train_model(self,num_labels,baseModel='./model/bert_uncased_L-4_H-512_A-8'):
        model = AutoModelForSequenceClassification.from_pretrained(baseModel,num_labels=num_labels)
        # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        model.to(self.device)
        
        #optimizer
        optimizer = AdamW(model.parameters(), lr=self.learning_rate)
        
        # train_test = Train_test()

        tqdm.pandas()

        for name, param in model.named_parameters():
            if 'classifier' not in name: # classifier layer
                param.requires_grad = False
            else:
                param.requires_grad = True
        
        
        for i in tqdm(range(self.num_of_epochs//2)):
            print("Epoch: #{}".format(i+1))
            self.train( model, optimizer)
            self.test(model)
            
        print("After Unfreezing the layer......................")

        for name, param in model.named_parameters():
        		param.requires_grad = True
                
        for i in tqdm(range(self.num_of_epochs//2, self.num_of_epochs)):
            print("Epoch: #{}".format(i+1))
            self.train(model, optimizer)
            self.test( model)
            
        # saving the model not saving optimizer
        self.model = model
        
    def predict(self,descriptions):
        
        # preprocess data
        X_test = descriptions.values.tolist()
        lent = len(X_test)
        X_test = predictor.tokenizer(X_test, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
        # test_loader = DataLoader(Dataset(X_test, y_test), batch_size=64, shuffle=True)
        
        test_ids = []
        test_attention_mask = []
        
        test_ids.append(X_test['input_ids'])
        test_attention_mask.append(X_test['attention_mask'])
        test_ids = torch.cat(test_ids, dim = 0)
        test_attention_mask = torch.cat(test_attention_mask, dim = 0)
        
        self.model.eval()
        with torch.no_grad():
            # predictions = predictor.model(**X_test)
            predictions = self.model(test_ids.to(self.device), token_type_ids = None, attention_mask = test_attention_mask.to(self.device))
        
        predictions_class = []
        for i in range(lent):
            predictions_class.append(predictions.logits.softmax(1)[i].argmax().item())
        
        return self.encoder.inverse_transform(predictions_class)
    



if __name__ == '__main__':
        
    data = pd.read_csv('../data/output/data_2019.csv')
    
    allcols = ['description','attackVector','attackComplexity','privilegesRequired','userInteraction','scope',
            'confidentialityImpact','integrityImpact','availabilityImpact','baseScore','baseSeverity','exploitabilityScore','impactScore']
    metc_clmn = ['attackVector','attackComplexity','privilegesRequired','userInteraction','scope',
            'confidentialityImpact','integrityImpact','availabilityImpact','baseScore','baseSeverity','exploitabilityScore','impactScore']
    data = data[allcols].dropna()
    
    # av_data = data[['description', 'attackVector']]
    X_train, X_test, y_train, y_test = train_test_split(data['description'], data['attackVector'], test_size=0.2, random_state=42)
    
    predictor = Predictor()    
    
    predictor.prepare_data(X_train, y_train)
    
    predictor.train_model(4)
    
    result = predictor.predict(X_test)

    y = y_test
    
    
    
    
#%% re indexing

# y_test.to_csv('av_reidx.csv',index=False)

# y_test = pd.read_csv('av_reidx.csv')


















